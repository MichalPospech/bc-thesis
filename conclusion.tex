
\chapwithtoc{Conclusion}

In this thesis reinforcement learning was formally introduced along with several methods such as Q-learning or REINFORCE. Then evolutionary algorithms were described and further expanded by evolutionary strategies including the introduction of CMA-ES. Following that OpenAI-ES was outlined, a evolutionary strategy that was designed for solving reinforcement learning tasks, high level of paralellisation and doesn't require differentiable policy as there is no differentiation needed unlike in traditional methods such as REINFORCE. This algorithm is further extended with novelty search which uses behavioral characteristics of evolved agents to evolve agents to behave in a different manner than their ancestors which should enable the agents to deal with falling into a local optima. Then the 2 test environment were outlined and data from experiments discussed. Finally some technical details about the implementation were explained.

Based on conducted experiments Slimevolley is a task that is hard to solve since only one method (CMA-ES) managed to do so and OpenAI-ES did so only few times and the performance varied heavily with seed. Cartpole has proven to be slightly easier with more methods being successful or would succeed given more time. Furthermore novelty search-based algorithm were found to be sensitive to quality of novelty calculation.

This work has shown that designing novelty search-based algorithms is mostly designing a good behavioral characteristic of an agent. Therefore this would be, along with more hyperparameter tuning, a possible direction to follow in research. Furthermore exploring performance of these methods on more environments would be beneficial as well. To get more accurate results on the two environments presented, spending more time with hyperparameter tuning and trying running longer experiments with a bigger population would help. Running more runs for each experiment would also be beneficial as it would create batter



