\chapter{Theory}
\label{chap:theory}

\section{Reinforcement learning}
\label{sec:reinf}
\begin{itemize}
    \item problem description (state space, action space, reward...)
    \item various methods \begin{itemize}
        \item value function
        \item criterion of optimality
        \item direct policy search (and various methods)
    \end{itemize}
\end{itemize}
\section{Evolutionary algorithms}
\label{sec:ea}
Evolutionary algorithms (EA) are a type of optimisation metaheuristics inspired by the process of bilogical evolution. At first a number of possible solutions to the problem at hand is generated (population) and each solution (individual) is encoded (via a domain-specific encoding) and evaluated giving us the value of its fitness. Then a new population is created using a crossover (combination) of 1 or more individuals which are selected at random with their fitness playing part in the chance to be selected for crossover. Each of the newly created individuals has a chance to be mutated via the mutation operator. Finally a new population is selected from offsprings and possibly the parents based of fitness and enters the next iteration of the EA. The algorithm repeats until the stop condition is met, usually a set number of iterations or small improvement of fitness between 2 generations. \cite{Rudolph2012} \cite{Vikhar2016} \todo{chybí algoritmus}
\section{Evolutionary strategies}
\label{sec:es}
Evolutionary strategies (ES) are a type of optimisation metaheuristic which further specialises EA and restricts their level of freedom. The selection for crossover is unbiased, mutation is parametrised and thus controllable, individuals which should be put to next generation are chosen ordinally based on fitness and individuals contain not only the problem solution but also control parameters.

More formally ES $(\mu,\kappa,\lambda,\rho)$ has $\mu$ individuals in each generation, which produces $\lambda$ offsprings, each created by crossover of $\rho$ individuals and each individual is able to survive for up to $\kappa$ generations. This notation further generalizes the old $(\mu,\lambda)$ and $(\mu+\lambda)$ notations, where the "," notation means $\kappa=1$ and "+" notation $\kappa=\infty$. \cite{Schwefel1995}\todo{víc rozepsat, ale jak moc? chybí algoritmus}
\begin{itemize}
    \item parametrization
    \item guidelines for operators (reachability, unbiasedness, control)
    \item covariances
\end{itemize}
     \cite{Rudolph2012}
\subsection{CMA-ES}
\label{subsec:cma-es}
TODO \cite{Hansen06}
\section{Evolutionary strategies as replacement for reinfocement learning}
\label{sec:es-reinf}
\begin{itemize}
    \item Evolution Strategies as a Scalable Alternative to Reinforcement Learning \cite{salimans2017} \begin{itemize}
        \item algorithm description
        \item comparison with RL
        \item paralellization
        \item smoothing in action vs. param space
    \end{itemize}
    \item Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents \cite{conti2018} \begin{itemize}
        \item novelty search
        \item ratio of fitness and novelty and its effects
    \end{itemize}
\end{itemize}