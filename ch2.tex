\chapter{Theory}
\label{chap:theory}

\section{Reinforcement learning}
\label{sec:reinf}
\begin{itemize}
    \item problem description (state space, action space, reward...)
    \item various methods \begin{itemize}
        \item value function
        \item criterion of optimality
        \item direct policy search (and various methods)
    \end{itemize}
\end{itemize}
\section{Evolutionary algorithms}
\label{sec:ea}
Evolutionary algorithms (EA) are a type of optimisation metaheuristics inspired by the process of bilogical evolution. At first a number of possible solutions to the problem at hand is generated (\emph{population}) and each solution (\emph{individual}) is encoded (via a domain-specific encoding) and evaluated giving us the value of its \emph{fitness}. Fitness is a function describing how good that particular individual is and it is everything that is needed for creation of  Then a new population is created using a \emph{crossover} (combination) of 1 or more individuals which are selected using the operator of \emph{parental selection}. Each of the newly created individuals has a chance to be mutated via the \emph{mutation} operator. Finally a new population is selected from \emph{offsprings} and possibly the parents based of fitness and enters the next iteration of the EA and the following generation is chosen using \emph{environmental selection} operator. The algorithm repeats until the stop condition is met, usually a set number of iterations or small improvement of fitness between 2 generations. 

There are many variands of EAs such as genetic algorithms (most common), genetic programming, evolutionary programming, neuroevolution or evolutionary strategies that are further described in following chapter. \cite{Rudolph2012} \cite{Vikhar2016}

\begin{algorithm}
    \begin{algorithmic}[1]
    \caption{Evolutionary algorithm}\label{alg:ea}
        \State initialize population $P^0$ with $n$ individuals
        \State set $t=0$
        \Repeat
            \State $Q^t = \{\}$
            \For{$i \in \{1\dots m\}$}
                \State $p_1,\dots,p_\rho = ParentalSelection(P^t)$
                \State $q = Crossover(p_1,\dots,p_\rho)$ 
                \State $q = Mutation(q)$ with chance $p$
                \State $Q^t = q \cup Q^t$
            \EndFor
            \State $P^{t+1} =EnvironmentalSelection(Q^t\cup  P^t)$
            \State increment $t$
        \Until{stop criterion fullfilled}
    \end{algorithmic}
    \end{algorithm}

\section{Evolutionary strategies}
\label{sec:es}
Evolutionary strategies (ES) are a type of optimisation metaheuristic which further specialises EA and restricts their level of freedom. The selection for crossover is unbiased, mutation is parametrised and thus controllable, individuals which should be put to next generation are chosen ordinally based on fitness and individuals contain not only the problem solution but also control parameters.

More formally ES $(\mu / \rho,\kappa,\lambda)$ has $\mu$ individuals in each generation, which produces $\lambda$ offsprings, each created by crossover of $\rho$ individuals and each individual is able to survive for up to $\kappa$ generations as described in algorithm \ref{alg:es}. This notation further generalizes the old $(\mu,\lambda)$ and $(\mu+\lambda)$ notations, where the "," notation means $\kappa=1$ and "+" notation $\kappa=\infty$. 
\begin{algorithm}
\begin{algorithmic}[1]
\caption{$(\mu / \rho,\kappa,\lambda)$-ES}
\label{alg:es}
    \State initialize population $P^0$ with $\mu$ individuals
    \State set age for each $p\in P^0$ to $1$
    \State set $t=0$
    \Repeat
        \State $Q^t = \{\}$
        \For{$i \in \{1\dots\lambda\}$}
            \State select $\rho$ parents $p_1,\dots,p_\rho \in P^t$ uniformly at random
            \State $q = variation(p_1,\dots,p_\rho)$ with age $0$
            \State $Q^t = q \cup Q^t$
        \EndFor
        \State $P^{t+1} =$ select $\mu$ best (wrt. fitness) individuals from $Q^t\cup \{p \in P^t: age(p)<\kappa\}$
        \State increment age by 1 for each $p \in P^{t+1}$
        \State increment $t$
    \Until{stop criterion fullfilled}
\end{algorithmic}
\end{algorithm}

To design an ES one must first select an appropriate representation for an individual and the most natural one is prefered in most cases, if all parameters are of one type (e.g. a real number) a simple vector will suffice, if the types are mixed, a tuple of vectors is required. This however causes an increased complexity of the variation operator.

As for design of the variation operator there are some guidelines that should be followed when designing it.
\begin{description}
    \item[Reachability] every solution should be reachable from any other solution in a finite number of applications of the variation operator with probability $p > 0$
    \item[Unbiasedness] the operator should not favour any particular subset of solution unless provided with information about problem at hand
    \item[Control] the operator should be parametrised in such way that the size of the distribution can be controlled (practice had shown that decreasing it as the optimal solution is being approached is necessary) 
\end{description}
\todo{kovariance}
\cite{Schwefel1995}
\cite{Rudolph2012}

\subsection{CMA-ES}
\label{subsec:cma-es}
TODO \cite{Hansen06}
\section{Evolutionary strategies as replacement for reinfocement learning}
\label{sec:es-reinf}
\todo{uvod}
\subsection{OpenAI Evolutionary Strategy}

Compared to reinfocement learning using evolutionary strategies have the advantage of not needing a gradient of the policy performance. Also as the state transition function is not known  the gradient can't be computed using backpropagation-like algorithm. Thus some noise needs to be added to make the problem smooth and the gradient to be estimable. Here is where reinfocement learning and evolutionary strategies differ, reinfocement learning adds noise in the action space (actions are chosen from a distribution) while evolutionary strategies add noise in the parameter space (parameters perturbed while actions are deterministic).



Not requiring backpropagation has several advantages over other RL methods. First the amount of computation necessary for one episode of ES is much lower (about one third, potentially even less for memory usage). Not calculating gradient using analytic methods also protects these methods from suffering from \emph{exploding gradient} which is a common issue with recurrent neural networks. And last, the network can contain elements that are not differentiable such as hard attention. 

As the ES could be seen as method for computing a derivative estimate using finite differences in randomly chosen direction it would suggest that it would scale poorly with dimensions of parameters $\theta$ same as the finite differences method. In theory the number of necessary optimisation steps should scale linearly with the dimension. That however doesn't mean that larger networks optimised using ES will perform worse than smaller ones, that depends on the difficulty (intrinsic dimension) of the problem. The network will perform the same however it will take more optimisation steps to do so. 

In practice ES performs slightly better on larger networks and it is hypothesised that it is for the same reason as why it is easier to optimise large networks using standard gradient based methods: larger networks have fewer local minima. 

Due to perturbing the parameters and not the actions ES are invariant to the frequency at which the agent acts in the envirionment. Tradtional MDP-based reinforcement learning methods rely on \emph{frameskip} as one their parameters that is crucial to get right for the optimization to be successful. While this is solvable for problems that do not require long term planning and actions, long term strategic behaviour poses a challenge and reinfocement learning needs hiearchy to be succesful unlike evolutionary strategy.
\begin{itemize}
    \item Evolution Strategies as a Scalable Alternative to Reinforcement Learning \cite{salimans2017} \begin{itemize}
        \item algorithm description
        \item comparison with RL
        \item paralellization
    \end{itemize}
    \item Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents \cite{conti2018} \begin{itemize}
        \item novelty search
        \item ratio of fitness and novelty and its effects
    \end{itemize}
\end{itemize}