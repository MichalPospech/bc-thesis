\chapter{Evolutionary strategies in reinforcement learning}
\label{ch:es-rl}
\label{sec:es-reinf}

Black-box optimisation is an alternative approach to solving RL tasks also known as Direct policy search or neurevolution when applied to neural networks. It has several attractive properties such as indifferenco to distribution of rewards, no need for backpropagation and tolerance of arbitrarily long episodes.

Compared to reinfocement learning using evolutionary strategies has the advantage of not needing a gradient of the policy performance. Also as the state transition function is not known  the gradient can't be computed using backpropagation-like algorithm. Thus some noise needs to be added to make the problem smooth and the gradient to be estimable. Here is where reinfocement learning and evolutionary strategies differ, reinfocement learning adds noise in the action space (actions are chosen from a distribution) while evolutionary strategies add noise in the parameter space (parameters perturbed while actions are deterministic).

Not requiring backpropagation has several advantages over other RL methods. First the amount of computation necessary for one episode of ES is much lower (about one third, potentially even less for memory usage). Not calculating gradient using analytic methods also protects these methods from suffering from \emph{exploding gradient} which is a common issue with recurrent neural networks. And last, the network can contain elements that are not differentiable such as hard attention.

Also ES are easily paralellisable. First, contrary to RL, where value function is inherently linear procedure and has to be performed more times to improve a given policy. Furthermore it operates on whole whole episodes, therefore it does not require frequent commuincation between workers. And finally, as the only information received by each worker is the reward, it is possible to communicate only the reward inbetween workers. That, however, requires synchronising seeds known to other workers beforehand and recreating perturbations based on that information. Thus the required is extremely low comapred to communication of whole gradients which would be required for paralellisation of a policy gradient based algorithm.

\section{OpenAI Evolutionary Strategy}
\label{subsec:openai-es}
These ideas are explored in OpenAI-ES algorithm. The agent acting in the envrionment is represented be a policy $\pi_\theta$  with parameter vector $\theta$ and $f(\cdot)$ is the reward returned by the environment. As we need to introduce some noise, the population is distribution $p_@y$ is instantiated as an an isotropic multivariate Gaussian with mean $\psi$ and covariance $\sigma^2I$. Then $\mathbb{E}_{\theta\sim p_\psi}f(\theta) = \mathbb{E}_{\epsilon\sim N(0,I)}f(\theta+\sigma\epsilon)$. Thus the gradient approximation is calculated as follows:
\begin{equation}
    \nabla_{\theta}\mathbb{E}_{\theta\sim p_\psi}f(\theta) =\nabla_{\theta}\mathbb{E}_{\epsilon\sim N(0,I)}f(\theta+\sigma\epsilon)\approx \frac{1}{n\sigma}\sum_{i=1}^n f(\theta_i)\epsilon_i,  
\end{equation}
where $\theta_i = \theta + \sigma\epsilon_i, \epsilon_i\sim N(0,I)$ and $n$ is the population size.

The resulting algorithm uses SGD (or Stochastic Gradient Ascent - SGA in this case) or other gradient based optimisation technique for parameter vector $\theta$ update.

\begin{algorithm}[h]
    \begin{algorithmic}[1]
    \caption{OpenAI-ES}
    \label{alg:openai-es}
        \State \textbf{Input}: Learning rate $\alpha$, noise standard deviation $\sigma$, initial policy parameters $\theta_0$, $n$ number of workers
        \State initialize $n$ workers with known random seeds, and initial parameters $\theta_0$
        \State set $t=0$
        \Repeat
            \For{$i \in \{1\dots n\}$}
                \State sample $\epsilon_i \sim \mathcal{N}(0,I)$ 
                \State compute returns $f_i = f(\theta_t+\sigma\epsilon_i)$
            \EndFor
            \State Send all scalar returns $f_i$ from each worker to every other worker
            \For{$i \in \{1\dots n\}$}
                \State reconstruct all perturbations $\epsilon_j $ for $j \in \{1,\dots,n\}$ using known random seeds
                \State set $\theta_{t+1} = \theta_t + \alpha \frac{1}{n\sigma}\sum_{j=1}^nf_i\epsilon_i$
            \EndFor
            \State increment $t$
        \Until{stop criterion fullfilled}
    \end{algorithmic}
\end{algorithm}

As the ES could be seen as method for computing a derivative estimate using finite differences in randomly chosen direction it would suggest that it would scale poorly with dimensions of parameters $\theta$ same as the finite differences method. In theory the number of necessary optimisation steps should scale linearly with the dimension. That however doesn't mean that larger networks optimised using ES will perform worse than smaller ones, that depends on the difficulty (intrinsic dimension) of the problem. The network will perform the same however it will take more optimisation steps to do so. 

In practice ES performs slightly better on larger networks and it is hypothesised that it is for the same reason as why it is easier to optimise large networks using standard gradient based methods: larger networks have fewer local minima. 

Due to perturbing the parameters and not the actions ES are invariant to the frequency at which the agent acts in the envirionment. Tradtional MDP-based reinforcement learning methods rely on \emph{frameskip} as one their parameters that is crucial to get right for the optimization to be successful. While this is solvable for problems that do not require long term planning and actions, long term strategic behaviour poses a challenge and reinfocement learning needs hiearchy to be succesful unlike evolutionary strategy. \cite{salimans2017}


\section{Novelty search}

While the main drive in of improvement in ES is the value of fitness (how "good" the result is), novelty search takes a different approach. Novelty search is focused on finding different solutions, as it is inspired by nature's drive towards diversity. Each policy has its novelty calculated with respect to previous policies and search is directed to parts of search space with high novelty. This approach makes it less succeptible to local optima created by deceptive rewards than reward-based method. 

Each policy $\pi$ gets assigned its domain-dependent behavorial characteristics $b(\pi)$ (e.g. final position of the agent) and it is added to an archive set $A$ of characteristics of previous policies. Then the novelty $N(b(\pi_\theta), A)$ is calculated as average distance from $k$ nearest neighbours from the archive set $A$.
\begin{equation}
    \begin{gathered}        
    N(\theta,A) = N(b(\pi_\theta),A)=\frac{1}{\left\lvert S\right\rvert }\sum_{j\in S} \left\lVert(\pi_\theta)-b(\pi_j) \right\rVert_2  \\
    S = kNN(b(\pi_\theta),A)
\end{gathered} 
\end{equation}
\subsection{Combination with evolution strategies}

To find and follow the gradient of expected novelty with respected to $\theta^t$ we use the framework outlined in \ref{subsec:openai-es}. 

With archive $A$ and sampled parameters $\theta_t^i=\theta_t + \sigma\epsilon_i$, the gradient estimate can be calculated via following formula:
\begin{equation}
    \label{nes:grad}
    \nabla_{\theta_t}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)} [ N(\theta_t + \sigma\epsilon)|A]\approx \frac{1}{n\sigma}\sum_{i=1}^n N(\theta_t^i,A)\epsilon_i 
\end{equation}

It is possible because archive $A$ is fixed during one iteration and is updated only at the end. Only characteristics corresponding to each $\theta^t$ are added to $A$, as adding each sampled would cause the archive $A$ to inflate too much increasing the complexity of calculation of nearest-neighbours.

To encourage additional diversity an initial meta-population of $M$, selection of $M$ is domain dependent, agents is created. While it is possible to optimise the behaviour of a single agent and reward it for behaving differently than its ancestors, this way we get the benefits of population-based exploration. \todo{Describe?} Each agent has parameters $\theta^m$ and is being rewarded for behaviour different from all prior agents, thus we get $M$ differently behaving policies.

$M$ random parameter vectors are initialised and in each iteration one is selected to be updated. The selection probability is proportional to its novelty.
\begin{equation}
    P(\theta^m) = \frac{N(\theta^m,A)}{\sum_{i=1}^M N(\theta^i,A)}
\end{equation}

To perform the update step, we need to calculate the gradient estimate of expected novelty with respect to $\theta^m_t$ using equation \ref{nes:grad} where $n$ is the number of perturbations. When we get the gradient estimate we use SGD (or in this case Stochastic Gradient Ascent) with learning rate $\alpha$ to update the parameters $\theta^m$
\begin{equation}
    \label{eq:ns-es-update}
    \theta^m_{t+1}\coloneqq\theta^m + \alpha \frac{1}{n\sigma}\sum_{i=1}^n N(\theta_t^{i,m},A)\epsilon_i.
\end{equation}
 After updating the individual, a new behavorial characteristics $b(\pi_{\theta^m_{t+1}})$ is calculated and added to the archive $A$. 

This process is repeated for a predetermined number of times as novelty search is not supposed to converge to a "best" solution and returns the best performing policy which is being preserved during the run of the algorithm.

\begin{algorithm}[h]
    \begin{algorithmic}[1]
    \caption{NS-ES}
    \label{alg:ns-es}
        \State \textbf{Input}: Learning rate $\alpha$, noise standard deviation $\sigma$, initial policy parameters $\theta_0$, $n$ number of workers, $T$ number of iterations
        \State initialize $n$ workers with known random seeds, empty archive $A$, and initial parameters $\{\theta^1_0,\dots, \theta^M_0\}$
        \State set $t=0$
        \For{$i \in \{1\dots M\}$}
            \State calculate $b(\pi_{\theta^i_0})$
            \State add $b(\pi_{\theta^i_0})$ to $A$
        \EndFor
        \For{$t \in \{0\dots T-1\}$}
            \State sample $\theta_t^m$ from $\{\theta^1_t,\dots, \theta^M_t\}$
            \For{$i \in \{1\dots n\}$}
                \State sample $\epsilon_i \sim \mathcal{N}(0,I)$ 
                \State compute characteristics $b(\theta^m_t+\sigma\epsilon_i)$
                \State compute $n_i = N(b(\theta^m_t+\sigma\epsilon_i), A)$
            \EndFor
            \State Send all novelties $n_i$ from each worker to every other worker
            \For{$i \in \{1\dots n\}$}
                \State reconstruct all perturbations $\epsilon_j $ for $j \in \{1,\dots,n\}$ using known random seeds
                \State set $\theta^m_{t+1} = \theta_t^m + \alpha \frac{1}{n\sigma}\sum_{j=1}^nn_i\epsilon_i$
                \State add $b(\theta^m_{t+1})$ to $A$
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}


\subsection{Combination with reward based exploration}
While \emph{NS-ES} helps agents avoid local optima and deceptive reward signals, it also completly discards reward which might cause the performance to suffer. Therefore NSR-ES, an improved version of NS-ES, uses both reward (fitness) and novelty for computation of the update step. NSR-ES is in many ways similar to NS-ES, it calculates both novelty and reward at once and it operates on entire episodes. The only difference is, that the calculation of the gradient estimate is based on the average of reward and novelty. 

Specifically, for parameter vector $\theta_t^{m,i} = \theta_t^m+\sigma\epsilon_i$ we calculate the reward $f(\theta_t^{m,i})$ and novelty $N(\theta_t^{m,i},A)$, rank-normalise both values independently (as both values usually have completely different scales), calculate the average and set it as weight for corresponding $\epsilon_i$ for gradient estimation. Then the estimated gradient is used to update the parameter vector via SGD (or other gradient based optimisation method) simirally as in equation \ref{eq:ns-es-update}:
\begin{equation}
    \theta^m_{t+1}\coloneqq\theta^m + \alpha \frac{1}{n\sigma}\sum_{i=1}^n \dfrac{N(\theta_t^{i,m},A)+f(\theta_t^{i,m})}{2}\epsilon_i.
\end{equation}

Intuitively, following the approximated gradient based on both novelty and reward directs the search areas of the parameter-space with both high novelty and reward. This can, however, be improved further.

\begin{algorithm}[h]
    \begin{algorithmic}[1]
    \caption{NSR-ES}
    \label{alg:nsr-es}
        \State \textbf{Input}: Learning rate $\alpha$, noise standard deviation $\sigma$, initial policy parameters $\theta_0$, $n$ number of workers, $T$ number of iterations
        \State initialize $n$ workers with known random seeds, empty archive $A$, and initial parameters $\{\theta^1_0,\dots, \theta^M_0\}$
        \State set $t=0$
        \For{$i \in \{1\dots M\}$}
            \State calculate $b(\pi_{\theta^i_0})$
            \State add $b(\pi_{\theta^i_0})$ to $A$
        \EndFor
        \For{$t \in \{0\dots T-1\}$}
            \State sample $\theta_t^m$ from $\{\theta^1_t,\dots, \theta^M_t\}$
            \For{$i \in \{1\dots n\}$}
                \State sample $\epsilon_i \sim \mathcal{N}(0,I)$ 
                \State compute characteristics $b(\theta^m_t+\sigma\epsilon_i)$
                \State compute $n_i = N(b(\theta^m_t+\sigma\epsilon_i), A)$
                \State compute $f_i = f(\theta^m_t+\sigma\epsilon_i)$
            \EndFor
            \State Send all novelties and rewards $n_i, f_i$ from each worker to every other worker
            \For{$i \in \{1\dots n\}$} 
                \State reconstruct all perturbations $\epsilon_j $ for $j \in \{1,\dots,n\}$ using known random seeds
                \State set $\theta^m_{t+1} = \theta_t^m + \alpha \frac{1}{n\sigma}\sum_{j=1}^n\frac{n_i+f_i}{2}\epsilon_i$
                \State add $b(\theta^m_{t+1})$ to $A$
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

While NSR-ES uses a linear combination of reward and novelty to approximate that is static for the whole duration of the training. Contrary to that NSRAdapt-ES (NSRA-ES) dynamically changes the ratio of reward gradient $f(\theta_t^{i,m}$ and novelty gradient $N(\theta_t^{i,m},A)$ based on how the training is currently progressing. This way, it will give more weight to the reward (thus following the performance gradient) when making progress and more weight to the novelty (following novelty gradient) when stuck in a local optima to give more incentive to find different approaches. 

Formally, a parameter $w$ is used to control the ratio of reward and novelty used for calculation of gradient estimate. For a specific $w$ at a given generation parameter vector $\theta_t^m$ is updated (SGD used) via following expression: 
\begin{equation}
    \theta^m_{t+1}\coloneqq\theta^m_t + \alpha \frac{1}{n\sigma}\sum_{i=1}^n (1-w)N(\theta_t^{i,m},A)\epsilon_i+wf(\theta_t^{i,m})\epsilon_i.
\end{equation}

\begin{algorithm}[h]
    \begin{algorithmic}[1]
    \caption{NSRA-ES}
    \label{alg:nsra-es}
        \State \textbf{Input}: Learning rate $\alpha$, noise standard deviation $\sigma$, initial policy parameters $\theta_0$, $n$ number of workers, $T$ number of iterations
        \State initialize $n$ workers with known random seeds, empty archive $A$, and initial parameters $\{\theta^1_0,\dots, \theta^M_0\}$
        \State set $t=0, t_{best}=0, f_{best}=-\infty$
        \For{$i \in \{1\dots M\}$}
            \State calculate $b(\pi_{\theta^i_0})$
            \State add $b(\pi_{\theta^i_0})$ to $A$
        \EndFor
        \For{$t \in \{0\dots T-1\}$}
            \State sample $\theta_t^m$ from $\{\theta^1_t,\dots, \theta^M_t\}$
            \For{$i \in \{1\dots n\}$}
                \State sample $\epsilon_i \sim \mathcal{N}(0,I)$ 
                \State compute characteristics $b(\theta^m_t+\sigma\epsilon_i)$
                \State compute $n_i = N(b(\theta^m_t+\sigma\epsilon_i), A)$
                \State compute $f_i = f(\theta^m_t+\sigma\epsilon_i)$
            \EndFor
            \State Send all novelties and rewards $n_i, f_i$ from each worker to every other worker
            \For{$i \in \{1\dots n\}$} 
                \State reconstruct all perturbations $\epsilon_j $ for $j \in \{1,\dots,n\}$ using known random seeds
                \State set $\theta^m_{t+1} = \theta_t^m + \alpha \frac{1}{n\sigma}\sum_{j=1}^n\frac{n_i+f_i}{2}\epsilon_i$
                \State add $b(\theta^m_{t+1})$ to $A$
            \EndFor
        \If{$f(\theta^m_{t+1}) > f_{best}$}
            \State $w=max(1, w+\delta_w)$
            \State $f_{best} = f(\theta^m_{t+1})$
            \State $t_{best} = 0$
        \Else
            \State $t_{best}+=1$
        \EndIf
        \If{$t_{best}\ge t_w$}
            \State $w=min(0, w-\delta_w)$
            \State $t_{best} = 0$
        \Else
        \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}


\cite{conti2018} 
