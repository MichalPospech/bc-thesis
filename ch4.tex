\chapter{Technical implementation}

All source code used in this thesis along with sample configuration files and instructions can be found at \url{https://github.com/MichalPospech/bc-project}.

The experiments were run using modified the \texttt{estool} tool, written in the Python programming language, which was originally used for an article about evolutionary strategies \cite{ha2017evolving}. Another option was to use implementation by Uber available at \url{https://github.com/uber-research/deep-neuroevolution} however it is much more complex (uses redis for inter-worker communication) and thus would be harder to extend.

\texttt{estool} makes use of the master-slave paradigm with one thread controlling the evolution and slaves only evaluating the candidate solutions. Communication between workers is done using \texttt{MPI} framework. The slave workers accept 3 types of commands from the master worker: \begin{itemize}
    \item \texttt{eval} - tells the slave to evaluate the solutions that will be sent next and send the results back to the master,
    \item \texttt{archive} - tells the slave to add the behavioral characteristic that will be sent next to the archive used for novelty calculation,
    \item \texttt{kill} - tells the slave to stop.
\end{itemize}

\texttt{estool} was extended to support novelty search by making a new class \texttt{NSAbstract} which encapsulates logic that all novelty search based algorithms (NS-ES, NSR-ES and NSRA-ES) have in common and classes for specific algorithms inherit from it and implement the algorithm-specific behaviour. Another modification necessary to enable usage of novelty was modification of the communication protocol between master and slaves, before it could handle only sending solutions from master to slaves and sending rewards back. Thus the aforementioned commands were introduced.

During training solution is evaluated more times (configured with parameter \texttt{num\_episode}) to get a better estimate of particular solution's performance because it varies based on selected seed. Similar approach is used for handling behavioural characteristics of solutions. One characteristic consists of \texttt{num\_episode} vectors and to calculate distance between two characteristics mean of all pairwise distances is used to account for strong dependence on seed. During evaluation of current solution, not used in training, an equivalent of whole population with each individiual being the solution to evaluate is created and gets evaluated to receive a more robust estimate of current performance.

However there is a slight deviation from the algorithms outlined in chapter \ref{ch:es-rl}. There only the results of solution evalution (and characteristics of new solutions in case of an algorithm that utilises novelty) are communicated between workers and all parameter perturbations are done locally based on shared seeds while \texttt{estool} does all perturbations in the master worker and solutions are sent, along with seeds for the task, to slave workers only for evaluation. This impacts the algorithm performance as the communication overhead increases with larger population and larger solutions (parameter vectors for policies) while the quality of generated solutions is not affected at all because the way new parameter vectors $\theta$ are calculated doesn't change.

As the project uses MPI for multiprocessing, running it is slightly more complex than running an ordinary Python project. It needs to be run using \texttt{mpiexec} as follows.
\begin{verbatim}
    mpiexec -n NUM_CPU python -m mpi4py train.py -f CONFIG
\end{verbatim}

Parameter \texttt{NUM\_CPU} must be at least 2 (due to the the master-slave paradigm) and at most the physical number of cores/nodes and \texttt{CONFIG} is a path to JSON configuration file for the experiment. Further details and instructions can be found in the \texttt{Readme.md} file in the project repository at \url{https://github.com/MichalPospech/bc-project}.
