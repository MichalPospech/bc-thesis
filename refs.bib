@article{Brown885,
  author    = {Brown, Noam and Sandholm, Tuomas},
  title     = {Superhuman AI for multiplayer poker},
  volume    = {365},
  number    = {6456},
  pages     = {885--890},
  year      = {2019},
  doi       = {10.1126/science.aay2400},
  publisher = {American Association for the Advancement of Science},
  abstract  = {Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold{\textquoteright}em poker. However, poker games usually include six players{\textemdash}a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold{\textquoteright}em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker.Science, this issue p. 885; see also p. 864In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold{\textquoteright}em poker, the most popular form of poker played by humans.},
  issn      = {0036-8075},
  url       = {https://science.sciencemag.org/content/365/6456/885},
  eprint    = {https://science.sciencemag.org/content/365/6456/885.full.pdf},
  journal   = {Science}
}


@misc{conti2018,
  title         = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents},
  author        = {Edoardo Conti and Vashisht Madhavan and Felipe Petroski Such and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  year          = {2018},
  eprint        = {1712.06560},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@article{ha2017evolving,
  title   = {Evolving Stable Strategies},
  author  = {Ha, David},
  journal = {blog.otoro.net},
  year    = {2017},
  url     = {http://blog.otoro.net/2017/11/12/evolving-stable-strategies/}
}

@misc{Hansen06,
  author = {Nikolaus Hansen},
  title  = {The CMA Evolution Strategy: A Comparing Review},
  year   = {2006}
}

@misc{hansen2016cma,
  title         = {The CMA Evolution Strategy: A Tutorial},
  author        = {Nikolaus Hansen},
  year          = {2016},
  eprint        = {1604.00772},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@article{LISSA2021100043,
  title    = {Deep reinforcement learning for home energy management system control},
  journal  = {Energy and AI},
  volume   = {3},
  pages    = {100043},
  year     = {2021},
  issn     = {2666-5468},
  doi      = {https://doi.org/10.1016/j.egyai.2020.100043},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666546820300434},
  author   = {Paulo Lissa and Conor Deane and Michael Schukat and Federico Seri and Marcus Keane and Enda Barrett},
  keywords = {Deep reinforcement learning, Residential home energy management, Demand response, Autonomous control},
  abstract = {The use of machine learning techniques has been proven to be a viable solution for smart home energy management. These techniques autonomously control heating and domestic hot water systems, which are the most relevant loads in a dwelling, helping consumers to reduce energy consumption and also improving their comfort. Moreover, the number of houses equipped with renewable energy resources is increasing, and this is a key element for energy usage optimization, where coordinating loads and production can bring additional savings and reduce peak loads. In this regard, we propose the development of a deep reinforcement learning (DRL) algorithm for indoor and domestic hot water temperature control, aiming to reduce energy consumption by optimizing the usage of PV energy production. Furthermore, a methodology for a new dynamic indoor temperature setpoint definition is presented, thus allowing greater flexibility and savings. The results show that the proposed DRL algorithm combined with the dynamic setpoint achieved on average 8% of energy savings compared to a rule-based algorithm, reaching up to 16% of savings over the summer period. Moreover, the users’ comfort has not been compromised, as the algorithm is calibrated to not exceed more than 1% of the time out the specified temperature setpoints. Additional analysis shows that further savings could be achieved if the time out of comfort is increased, which could be agreed according to users’ needs. Regarding demand side management, the DRL control shows efficiency by anticipating and delaying actions for a PV self-consumption optimization, performing over 10% of load shifting. Finally, the renewable energy consumption is 9.5% higher for the DRL-based model compared to the rule-based, which means less energy consumed from the grid.}
}
@misc{openai2019dota,
  title         = {Dota 2 with Large Scale Deep Reinforcement Learning},
  author        = {OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique P. d. O. Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  year          = {2019},
  eprint        = {1912.06680},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{openai2019solving,
  title         = {Solving Rubik's Cube with a Robot Hand},
  author        = {OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and Nikolas Tezak and Jerry Tworek and Peter Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
  year          = {2019},
  eprint        = {1910.07113},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inbook{Rudolph2012,
  author    = {Rudolph, G{\"u}nter},
  editor    = {Rozenberg, Grzegorz
and B{\"a}ck, Thomas
and Kok, Joost N.},
  title     = {Evolutionary Strategies},
  booktitle = {Handbook of Natural Computing},
  year      = {2012},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {673--698},
  abstract  = {Evolutionary strategies (ES) are part of the all-embracing class of evolutionary algorithms (EA). This chapter presents a compact tour through the developments regarding ES from the early beginning up to the recent past before advanced techniques and the state-of-the-art techniques are explicated in detail. The characterizing features that distinguish ES from other subclasses of EA are discussed as well.},
  isbn      = {978-3-540-92910-9},
  doi       = {10.1007/978-3-540-92910-9_22},
  url       = {https://doi.org/10.1007/978-3-540-92910-9_22}
}
@misc{salimans2017,
  title         = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  author        = {Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
  year          = {2017},
  eprint        = {1703.03864},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{Schrittwieser2020,
  author   = {Schrittwieser, Julian
and Antonoglou, Ioannis
and Hubert, Thomas
and Simonyan, Karen
and Sifre, Laurent
and Schmitt, Simon
and Guez, Arthur
and Lockhart, Edward
and Hassabis, Demis
and Graepel, Thore
and Lillicrap, Timothy
and Silver, David},
  title    = {Mastering Atari, Go, chess and shogi by planning with a learned model},
  journal  = {Nature},
  year     = {2020},
  month    = {Dec},
  day      = {01},
  volume   = {588},
  number   = {7839},
  pages    = {604-609},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
  issn     = {1476-4687},
  doi      = {10.1038/s41586-020-03051-4},
  url      = {https://doi.org/10.1038/s41586-020-03051-4}
}

@inproceedings{Schwefel1995,
  author    = {Schwefel, Hans-Paul
and Rudolph, G{\"u}nter},
  editor    = {Mor{\'a}n, Federico
and Moreno, Alvaro
and Merelo, Juan Juli{\'a}n
and Chac{\'o}n, Pablo},
  title     = {Contemporary evolution strategies},
  booktitle = {Advances in Artificial Life},
  year      = {1995},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {891--907},
  abstract  = {After an outline of the history of evolutionary algorithms, a new ($\mu$, $\kappa$, $\lambda$, $\rho$) variant of the evolution strategies is introduced formally. Though not comprising all degrees of freedom, it is richer in the number of features than the meanwhile old ($\mu$, $\lambda$) and ($\mu$+$\lambda$) versions. Finally, all important theoretically proven facts about evolution strategies are briefly summarized and some of many open questions concerning evolutionary algorithms in general are pointed out.},
  isbn      = {978-3-540-49286-3}
}

@phdthesis{Sehnke2012,
  author   = {Sehnke, Frank},
  title    = {Parameter Exploring Policy Gradients and their Implications},
  type     = {Dissertation},
  school   = {Technische Universität München},
  address  = {München},
  year     = 2012,
  keywords = {Reinforcement Learning, Policy Gradients, Parameter Exploration, Robotics}
}

﻿@misc{slimevolleygym,
  author       = {David Ha},
  title        = {Slime Volleyball Gym Environment},
  year         = {2020},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/hardmaru/slimevolleygym}}
}

@book{Sutton1998,
  added-at  = {2019-07-13T10:11:53.000+0200},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition   = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords  = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title     = {Reinforcement Learning: An Introduction},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  year      = {2018 }
}
@inproceedings{Vikhar2016,
  author    = {P. A. {Vikhar}},
  booktitle = {2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC)},
  title     = {Evolutionary algorithms: A critical review and its future prospects},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {261-265},
  doi       = {10.1109/ICGTSPICC.2016.7955308}
}
@misc{kiran2021deep,
      title={Deep Reinforcement Learning for Autonomous Driving: A Survey}, 
      author={B Ravi Kiran and Ibrahim Sobh and Victor Talpaert and Patrick Mannion and Ahmad A. Al Sallab and Senthil Yogamani and Patrick Pérez},
      year={2021},
      eprint={2002.00444},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{yu2020reinforcement,
      title={Reinforcement Learning in Healthcare: A Survey}, 
      author={Chao Yu and Jiming Liu and Shamim Nemati},
      year={2020},
      eprint={1908.08796},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}